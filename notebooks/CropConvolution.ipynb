{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image, ImageMath\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from glob import glob\n",
    "import imageio\n",
    "from copy import deepcopy\n",
    "import cv2\n",
    "import torchvision.models as models\n",
    "from utils import tri_mirror\n",
    "import splitfolders \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 35)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(train_path)), len(os.listdir(val_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val', 'train']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/yyelisieiev/luftr_data/CroppedNewRivets/\"\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "val_path = os.path.join(data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob(os.path.join(data_dir, \"*.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_splitter(list_to_split, ratio):\n",
    "    elements = len(list_to_split)\n",
    "    middle = int(elements * ratio)\n",
    "    return [list_to_split[:middle], list_to_split[middle:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = list_splitter(file_list, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in train:\n",
    "    image_name = image.split(\"/\")[-1]\n",
    "    shutil.move(image, os.path.join(train_path, image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in val:\n",
    "    image_name = image.split(\"/\")[-1]\n",
    "    shutil.move(image, os.path.join(val_path, image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f035748a6a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: shutil.move(os.path.join(data_dir, x), os.path.join(train_path, x)), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f03571de520>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: shutil.move(os.path.join(data_dir, x), os.path.join(val_path, x)), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 0 files [00:00, ? files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio(data_dir, seed=1337, ratio=(.8, .2), group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = imageio.mimread(\"../new_logs/ReLUMaxPoolingQuaterCrop/summaries/8.gif\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2b0681aeae8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RivetDataset(Dataset):\n",
    "    ANN_DIR = \"annotations\"\n",
    "    CATEGORY = \"rivet\"\n",
    "    \n",
    "    def __init__(self, config, transform, validation=False):\n",
    "        self.root_dir = config['data']['data_dir']\n",
    "        self.image_size = config['data']['image_size']\n",
    "        self.image_type = config['data']['image_type']\n",
    "        self.transform = transform\n",
    "        self.img_channels = 1\n",
    "\n",
    "        self.file_list = glob(os.path.join(self.root_dir, \"*\"))\n",
    "        self.feature_extractor = models.vgg16(pretrained=True).features.cuda()\n",
    "        self.feature_extractor.eval()\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = dict()\n",
    "        img_path = self.file_list[idx]\n",
    "        rivet = Image.open(img_path).resize((self.image_size, self.image_size))\n",
    "#         print(rivet.shape)\n",
    "        center = self.image_size // 2\n",
    "        cv = self.image_size // 4\n",
    "        \n",
    "        if self.image_type == \"Corrupted\":\n",
    "            rivet = np.array(rivet).astype(np.float32)\n",
    "            center_rivet = deepcopy(rivet[center - cv: center + cv, center - cv: center + cv])\n",
    "            rivet[center - cv: center + cv, center - cv: center + cv] = 0.0\n",
    "\n",
    "            rivet_rgb = np.zeros((rivet.shape[0], rivet.shape[0], 3))\n",
    "            rivet_rgb[:,:, 0] = rivet\n",
    "            rivet_rgb[:,:, 1] = rivet\n",
    "            rivet_rgb[:,:, 2] = rivet\n",
    "            rivet = Image.fromarray((rivet_rgb*255).astype(np.uint8))\n",
    "            center_rivet = Image.fromarray(center_rivet)\n",
    "                \n",
    "        elif self.image_type == \"TriMir\":\n",
    "            rivet = tri_mirror(rivet, center, cv)\n",
    "            \n",
    "        rivet = self.transform(rivet).unsqueeze(0).cpu()\n",
    "#         rivet = self.feature_extractor(rivet).view(-1, 1).detach().cpu().flatten()\n",
    "        center_rivet = self.transform(center_rivet)\n",
    "\n",
    "        rivet = rivet.unsqueeze(0)\n",
    "        center_rivet = center_rivet.unsqueeze(0)\n",
    "\n",
    "        X['Rivet'] = rivet\n",
    "        X['Center'] = center_rivet\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, overlap, bias=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / np.sqrt(fan_in)\n",
    "            torch.nn.init.torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, in_channels, in_h, in_w = x.shape\n",
    "\n",
    "        crop_start = (in_h // 4) - self.overlap\n",
    "        start_idx = in_h * crop_start + crop_start\n",
    "        crop_size = (in_h // 2) + self.overlap * 2\n",
    "\n",
    "        out_h = ((in_h - self.kernel_size + 2 * self.padding) //self.stride + 1)\n",
    "        out_w = ((in_w - self.kernel_size + 2 * self.padding) //self.stride + 1)\n",
    "\n",
    "        unfold = torch.nn.Unfold(kernel_size=(self.kernel_size, self.kernel_size), dilation=self.dilation, padding=self.padding, stride=self.stride)\n",
    "        inp_unf = unfold(x)\n",
    "        crop_lst = []\n",
    "        for i in range(crop_size):\n",
    "            if i == 0:\n",
    "                crop_lst.append(torch.ones([batch_size, self.out_channels, inp_unf[:, :, :start_idx].shape[2]], dtype=torch.bool))\n",
    "            if i == crop_size - 1:\n",
    "                crop_lst.append(torch.zeros([batch_size, self.out_channels, inp_unf[:, :, start_idx:start_idx + crop_size].shape[2]], dtype=torch.bool))\n",
    "                crop_lst.append(torch.ones([batch_size, self.out_channels, inp_unf[:, :, start_idx+crop_size:(in_h**2)].shape[2]], dtype=torch.bool))\n",
    "                break\n",
    "            crop_lst.append(torch.zeros([batch_size, self.out_channels, inp_unf[:, :, start_idx:start_idx + crop_size].shape[2]], dtype=torch.bool))\n",
    "            crop_lst.append(torch.ones([batch_size, self.out_channels, inp_unf[:, :, start_idx + crop_size: start_idx + in_h].shape[2]], dtype=torch.bool))\n",
    "            start_idx += in_h\n",
    "\n",
    "        crop_indexes = torch.cat(crop_lst, axis=2)\n",
    "        out_unf = inp_unf.transpose(1, 2).matmul(self.weight.view(self.weight.size(0), -1).t())\n",
    "\n",
    "        if self.bias is None:\n",
    "            out_unf = out_unf.transpose(1, 2)\n",
    "        else:\n",
    "            print(out_unf.shape)\n",
    "            print(self.bias.shape)\n",
    "            out_unf = (out_unf + self.bias).transpose(1, 2)\n",
    "        out_unf = torch.where(crop_indexes, out_unf, torch.zeros(out_unf.shape, dtype=torch.float32))\n",
    "        print(out_unf.shape)\n",
    "        print(batch_size, self.out_channels, out_h, out_w)\n",
    "        out = out_unf.view(batch_size, self.out_channels, out_h, out_w)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            CropConv(in_channels=3, out_channels=128, kernel_size=3, stride=2, padding=1, dilation=1, overlap=0),\n",
    "            nn.ReLU(),\n",
    "            CropConv(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1, dilation=1, overlap=0),\n",
    "            nn.ReLU(),\n",
    "            CropConv(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1, dilation=1, overlap=0),\n",
    "            nn.ReLU(),\n",
    "\t\t\tCropConv(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1, dilation=1, overlap=0),\n",
    "            nn.ReLU(),\n",
    "            CropConv(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1, dilation=1, overlap=0),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 1, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conv = CropConv(in_channels=1, out_channels=4, kernel_size=3, stride=2, padding=1, dilation=1, overlap=0, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop size:  8\n",
      "torch.Size([1, 64, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([1, 4, 64])\n",
      "1 4 8 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 8])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_conv(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.5"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(16 - 3 + 2)/2 + 1\n",
    "# ((in_s - kernel_size + 2 * padding)/stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "config['data'][\"data_dir\"] = \"/home/yyelisieiev/luftr_data/cropped_rivets\"\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset = RivetDataset(config, transform, validation=False)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input:  torch.Size([1, 3, 64, 64])\n",
      "Crop size:  32\n",
      "torch.Size([1, 128, 1024])\n",
      "1 128 32 32\n",
      "Crop size:  16\n",
      "torch.Size([1, 256, 256])\n",
      "1 256 16 16\n",
      "Crop size:  8\n",
      "torch.Size([1, 256, 64])\n",
      "1 256 8 8\n",
      "Crop size:  4\n",
      "torch.Size([1, 256, 16])\n",
      "1 256 4 4\n",
      "Crop size:  2\n",
      "torch.Size([1, 256, 4])\n",
      "1 256 2 2\n",
      "torch.Size([1, 256, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for index, inputs in enumerate(dataloader):\n",
    "    model_input = inputs['Rivet'][0][0]\n",
    "    print(\"Model input: \", model_input.shape)\n",
    "    out = encoder(model_input)\n",
    "    print(out.shape)\n",
    "    break\n",
    "#         im.save(os.path.join(rivet_dir, f\"{index}_{image_idx}_rivet.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(out.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
